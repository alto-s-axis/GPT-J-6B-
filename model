// Update pip if not updated 
!pip insatll --update pip

// insatll transformers 
!pip install transformers

//install torch
!pip install torch

// !pip install transformers[torcch] // to install both at once
// import float 16 version of gpt j 6b for low spec systems
from transformers import GPTJForCausalLM

import torch        // impoert torch

// Download gpt model from hugging face 
model = GPTJForCausalLM.from_pretrained(
    "EleutherAI/gpt-j-6B",
		revision="float16",
		torch_dtype=torch.float16,
		low_cpu_mem_usage=True
)

// Save `GPT-J` using `torch.save' save it with `torch.save()`.
torch.save(model, "gptj.pt")

// import pipeline to load model
from transformers import pipeline
import torch

// load our GPT-J model with torch.load() to run
model = torch.load("gptj.pt")

// load tokenizer
tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-j-6B")

// create pipeline
gen = pipeline("text-generation",model=model,tokenizer=tokenizer,device=0)

// run model
gen("type here")  // type text to generate


